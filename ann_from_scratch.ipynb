{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_4pYmqiP7eU"
      },
      "source": [
        "# Artificial Neural Network from Scratch\n",
        "\n",
        "In this notebook we'll look get a good look implementing a simple neural network, or perceptron. Specifically, we will create a simple, single layer perceptron with only one feature for input.\n",
        "\n",
        "We'll build this from scratch, then compare it to logistic regression. After that, we'll use PyTorch's functionality to simplify everything - from calculating derivatives with the chain rule to the optimization itself.\n",
        "\n",
        "\n",
        "### 1: Generate Synthetic Data\n",
        "We'll first import the necessary Python modules and then generate synthetic data with appropriate size/dimensions. We'll also plot the generated data to see x and y. Note that the y outcomes/labels belong to one of two classes. The positive cases are shown in blue while negative cases are in red."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QB4IpTbHP7eV",
        "outputId": "c60cbddb-256b-42ba-b2c5-06e6b8d9277e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "N = 100 # total number of observations\n",
        "D_in = 1 # input dimension (i.e. dimension of a single observation's x vector)\n",
        "D_out = 1 # output dimension (i.e. y), so just 1 for this example\n",
        "random.seed(1)\n",
        "np.random.RandomState(1)\n",
        "\n",
        "# Create random input data and derive the 'true' labels/output\n",
        "x = np.random.randn(N, D_in) + 1\n",
        "def true_y(x_in, n_obs):\n",
        "    def addNoise(x):\n",
        "        if abs(x-1) < 1:\n",
        "            return 0.1\n",
        "        elif abs(x-1) < 0.1:\n",
        "            return 0.25\n",
        "        else:\n",
        "            return 0.01\n",
        "\n",
        "    return np.apply_along_axis(lambda x: [int(x[0] < 1) if random.random() < addNoise(x) else int(x[0] > 1)], 1, x_in)\n",
        "\n",
        "y = true_y(x, N).flatten()\n",
        "\n",
        "plt.scatter(x[y == 1,0], y[y == 1], c='blue', s=100, alpha=0.2)\n",
        "plt.scatter(x[y == 0,0], y[y == 0], c='red', s=100, alpha=0.2)\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend(('positive cases', 'negative cases'))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHZdwnSQP7eV"
      },
      "source": [
        "Let's quickly peek at the x and y data objects (i.e. numpy arrays) to see what their size, shape, and rank look like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEWKy_6WP7eV"
      },
      "outputs": [],
      "source": [
        "print(f\"y.size: {y.size}\")\n",
        "print(f\"y.shape: {y.shape}\")\n",
        "print(f\"y.ndim: {y.ndim}\")\n",
        "\n",
        "print(f\"x.size: {x.size}\")\n",
        "print(f\"x.shape: {x.shape}\")\n",
        "print(f\"x.ndim: {x.ndim}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcHpqc5GP7eW"
      },
      "source": [
        "### 2a. Fit Logistic Regression Model\n",
        "\n",
        "We've already seen that a single layer perceptron with a sigmoid output activation function is equivalent to logisitic regression. So, before we start with the perceptron let's quickly review logistic regression by fitting it to the simulated data we created above.  \n",
        "\n",
        "That is, let's find estimates for $\\beta_0$ and $\\beta_1$, which we'll need to be able to predict/estimate the probability that y is in the positive or negative class for any given x.\n",
        "\n",
        "In traditional statistics there is a large class of models known as [generalized linear models](https://en.wikipedia.org/wiki/Generalized_linear_model), which allow for a linear regression to be used to predict some transformation of the target variable. With logistic regression we transform the target variable, $Y$ (a binary 0 or 1) by looking at the log-odds of the expected value of $y_i$. If we let $p_i be the probability that $y_i$ equals 1, then the log-odds are as follows (note that the log-odds are formally known as the [logit](https://en.wikipedia.org/wiki/Logit) function):\n",
        "\n",
        "* $\\mathrm{logit}(p_i) = ln\\Big( \\frac{p_i}{1 - p_i} \\Big)= \\beta_0 + \\beta_1*x_i$\n",
        "\n",
        "Notice that the right side of the formula is in the familiar linear regression format of $\\beta$ coefficients and their corresponding $x$ covariates.\n",
        "\n",
        "When it comes time to make a prediction for a new $y_i$, we'll then use the inverse of the logit function, which happens to be the logistic, or sigmoid, function:\n",
        "\n",
        "* $p_i = \\mathrm{logit}^{-1}\\Big( \\mathrm{logit}(p_i) \\Big) = \\mathrm{logit}^{-1}(\\beta_0 + \\beta_1*x_i) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1*x_i)}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8yf_w8vP7eW"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "logreg_model = LogisticRegression(random_state=42, max_iter=100, tol=1e-3, solver='liblinear')\n",
        "logreg_model.fit(x, y)\n",
        "\n",
        "print(f\" beta0 = {logreg_model.intercept_[0]:.4f}\")\n",
        "print(f\" beta1 = {logreg_model.coef_[0][0]:.4f}\")\n",
        "y_pred = logreg_model.predict_proba(x)\n",
        "lr_loss = 1/N * np.square(y - y_pred[:,1]).sum()\n",
        "print(f\" loss (mse) = {lr_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvM3602NP7eW"
      },
      "source": [
        "To find those estimates scikit learn's LogisticRegression model solved an optimization problem that we are familiar with, which was to minimize the loss function. Recall that the mean-squared error loss function looks like this:\n",
        "\n",
        "* $\\mathrm{Loss}_{MSE} = \\frac{1}{N} \\sum_i^N (y_i - p_i)^2$\n",
        "\n",
        "\n",
        "Note that we've sometimes used $\\hat{y}_i$ instead of $p_i$, but they typically considered to be the same quantity (i.e. the direct output of the model).\n",
        "\n",
        "If we state $p_i$ in terms of our model parameters then, as seen above, we have the following:\n",
        "\n",
        "* $ \\mathrm{Loss}_{MSE} = \\frac{1}{N} \\sum_i^N \\Big( y_i - (1 + e^{-(\\beta_0 + \\beta_1*x_i)})^{-1} \\Big)^2$\n",
        "\n",
        "By writing the loss function as a function of our model parameters, we can then plot the surface of the loss with $\\beta_0$ and $\\beta_1$ on the x and y axes, respectively. We can then see where the optimal values might be. Remember that in practice, when we have hundreds, thousands, or more, model parameters, that it is impossible to plot the loss surface in this way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bKxcYsKP7eW"
      },
      "outputs": [],
      "source": [
        "#b1s = np.arange(6, -4.1, -0.5)\n",
        "#b0s = np.arange(-6, 4.1, 0.5)\n",
        "b1s = np.arange(9, -5, -0.25)\n",
        "b0s = np.arange(-9, 5, 0.25)\n",
        "surf = np.array( [[1/N * np.square(y - 1 / (1 + np.exp(-1 * (b1s[i]*x[:,0] + b0s[j])))).sum() for j in range(len(b0s))] for i in range(len(b1s))] )\n",
        "df = pd.DataFrame(surf, columns=b0s, index=b1s)\n",
        "p1 = sns.heatmap(df, cbar_kws={'label': 'loss'}, cmap=\"RdYlGn_r\")\n",
        "plt.title(\"Loss surface\")\n",
        "plt.xlabel(\"beta0\")\n",
        "plt.ylabel(\"beta1\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-GyUuKLP7eW"
      },
      "source": [
        "Notice above that there does not appear to be one unique pair of values for $\\beta_0$, $\\beta_1$ that will yield a minimum loss value. That is to say, there is no single point on the surface we plotted above where the loss has an obvivous global minimum. Instead, it looks as if the loss continues to decrease as $\\beta_0$ gets smaller and $\\beta_1$ gets larger.\n",
        "\n",
        "What would happen if we chose to use another loss function? Would the log-loss (aka binary cross-entropy) function look much different? The log-loss, or binary cross-entropy function is defined as follows.\n",
        "\n",
        "* $\\mathrm{Loss}_{BCE} = - \\sum_i^N \\Big( y_i * \\mathrm{ln}(\\hat{y}_i) + (1 - y_i)*\\mathrm{ln}(1-\\hat{y}_i) \\Big)$\n",
        "\n",
        "This is a different looking function altogether from the MSE. We would expect, however, that the surface would be similar though so that we would ultimately find similar parameter values. Let's plot this now to see. (Note that in practice the log-loss is the loss function typically used for logistic regression models.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaTZoO39P7eW"
      },
      "outputs": [],
      "source": [
        "#b1s = np.arange(9, -4.1, -0.5)\n",
        "#b0s = np.arange(-9, 4.1, 0.5)\n",
        "b1s = np.arange(9, -5, -0.25)\n",
        "b0s = np.arange(-9, 5, 0.25)\n",
        "surf = np.array( [[ -((y * np.log2(1 / (1 + np.exp(-1 * (b1s[i]*x[:,0] + b0s[j])))) + (1-y) * np.log2(1 - 1 / (1 + np.exp(-1 * (b1s[i]*x[:,0] + b0s[j])))))/100).sum() for j in range(len(b0s))] for i in range(len(b1s))] )\n",
        "df = pd.DataFrame(surf, columns=b0s, index=b1s)\n",
        "p1 = sns.heatmap(df, cbar_kws={'label': 'loss'}, cmap=\"RdYlGn_r\")\n",
        "plt.xlabel(\"beta0\")\n",
        "plt.ylabel(\"beta1\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hvg5Eu91P7eX"
      },
      "source": [
        "Although the two surfaces are noticeably distinct from one another, they do appear to agree on what values of the parameters will minimize the loss. It appears that in both cases the loss will be minimized when $\\beta_0$ is a small value (far to the left on the graph) and $\\beta_1$ is a large value (up high on the graph)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peACbmowP7eX"
      },
      "source": [
        "### 2b: Predict y for a Newly Observed x\n",
        "\n",
        "Let's see now what the model looks like using the parameter estimates found above (i.e. $\\beta_0 = -2.17$, $\\beta_1 = 2.55$). Next let's assume we are given a new observation to make a prediction for, and for this new observation we have $x = 1.5$. We can then predict whether it is a positive or negative case by evaluating the logistic function at $x = 1.2$. This yields $\\hat{y} \\approx 0.7$ (see below).\n",
        "\n",
        "Since $0.5$ is the typical cutoff value for predicting a positive (vs negative label), and $0.7 > 0.5$, we would likely predict this to be a positive case.\n",
        "\n",
        "__TIP:__ Try changing the values of the model parameters $\\beta_0$ and $\\beta_1$ below. Using the plots of the loss surfaces above, try choosing values of the model parameters look like they will yield an even lower point on the surface (i.e. smaller value of loss function). What happens to the plot below when doing so?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQUnAkwkP7eX"
      },
      "outputs": [],
      "source": [
        "b0 = -2.16\n",
        "b1 = 2.55\n",
        "\n",
        "plt.scatter(x[y == 1,0], y[y == 1], c='blue', alpha=0.4)\n",
        "plt.scatter(x[y == 0,0], y[y == 0], c='red', alpha=0.4)\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "\n",
        "x_new = 1.2\n",
        "y_hat_new = 1 / (1 + np.exp(-b0 - b1*x_new))\n",
        "plt.scatter(x_new, y_hat_new, color=\"green\")\n",
        "plt.legend(('positive cases', 'negative cases', 'new prediction'), loc='upper left')\n",
        "\n",
        "xes = np.arange(-4, 4, 0.2)\n",
        "plt.plot(xes, 1/(1 + np.exp(-b0 - b1*xes)), 'k--')\n",
        "\n",
        "plt.show()\n",
        "y_hat_new\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sT-C_KtZP7eX"
      },
      "source": [
        "### 2c. Why not use Accuracy for Optimization? (instead of MSE or BCE)\n",
        "\n",
        "At some point you may have asked yourself, \"_Why don't we just train our model by optimizing for accuracy?_\"\n",
        "\n",
        "Below is a plot of the surface when using accuracy as the function that the model will optimize for during training. Run the following cell and then answer the questions below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IOKqgZXP7eX"
      },
      "outputs": [],
      "source": [
        "b1s = np.arange(9, -5, -0.25)\n",
        "b0s = np.arange(-9, 5, 0.25)\n",
        "def accuracy(b0, b1):\n",
        "    # calculate the model output for this b0 and b1\n",
        "    y_hat = 1 / (1 + np.exp(-b0 - b1 * x.flatten()))\n",
        "    # convert the model output to binary predictions\n",
        "    y_hat = np.round(y_hat)\n",
        "    return np.sum(y == y_hat) / len(y)\n",
        "surf = np.array( [[ accuracy(b0s[j], b1s[i]) for j in range(len(b0s))] for i in range(len(b1s))] )\n",
        "df = pd.DataFrame(surf, columns=b0s, index=b1s)\n",
        "p1 = sns.heatmap(df, cbar_kws={'label': 'accuracy'}, cmap=\"RdYlGn_r\")\n",
        "plt.xlabel(\"beta0\")\n",
        "plt.ylabel(\"beta1\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dbkw7GAMP7eX"
      },
      "source": [
        "***\n",
        "### Q1: What is different about using accuracy versus a loss function (e.g. MSE or BCE)?\n",
        "(Hint: What are the ranges of values that accuracy takes on versus the loss functions? And, which are considered _good_ and which are considered _bad_?)\n",
        "\n",
        "\\<INPUT YOUR ANSWER TO Q1 HERE\\>\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRkbDkh-P7eX"
      },
      "source": [
        "***\n",
        "### Q2: Assuming you account for what is different about using accuracy versus a loss function, there is still something wrong with using accuracy. What is it? That is, why wouldn't you use accuracy to train a neural network model?\n",
        "(Hint: How does the accuracy surface differ from the MSE or BCE surfaces? What happens right around b=0, w=0?)\n",
        "\n",
        "\\<INPUT YOUR ANSWER TO Q2 HERE\\>\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ws7Io5pTP7eX"
      },
      "source": [
        "### 3. Simple Perceptron w/ NumPy\n",
        "\n",
        "Using the same $x$ and $y$ data as before we will now see how a simple perceptron network with a sigmoid activiation function is equivalent to the logistic regression model above. Rather than $\\beta_0$, $\\beta_1$, the parameters to be estimated are generally referred to as the weight and bias terms ($w$ and $b$, respectively). For this simple case they differ only in name though, such that:\n",
        "* $b = \\beta_0$\n",
        "* $w = \\beta_1$\n",
        "\n",
        "To begin we will manually train the perceptron using numpy and gradient descent to minimize the loss. This will involve taking the derivatives of $w$ and $b$ (using the chain rule) to calculate the gradients in an interative process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sge7TCo-P7eX"
      },
      "outputs": [],
      "source": [
        "# Randomly initialize parameters to be estimated\n",
        "#np.random.seed(42)\n",
        "w = np.random.randn(1)\n",
        "b = np.random.randn(1)\n",
        "\n",
        "# learning rate parameter\n",
        "learning_rate = 5e-1\n",
        "\n",
        "# keep track of loss to see how the optimization performs\n",
        "loss = []\n",
        "\n",
        "params = []\n",
        "\n",
        "# Begin gradient descent using all of the observations in each iteration\n",
        "for i in range(250):\n",
        "\n",
        "    params.append((w[0], b[0]))\n",
        "\n",
        "    # Forward pass: compute predicted y\n",
        "    lin_pred = w[0] * x[:,0] + b[0]\n",
        "    y_pred = 1 / (1 + np.exp(-1 * lin_pred))\n",
        "\n",
        "    # Compute and store loss, and print occassionally\n",
        "    loss.append(1/N * np.square(y - y_pred).sum())\n",
        "    if i % 50 == 0:\n",
        "        print(f\"iteration {i}: loss = {loss[i]:.4f}, w = {w[0]:.4f}, b = {b[0]:.4f}\")\n",
        "\n",
        "    # Backprop to compute gradients of w and b with respect to log loss\n",
        "    dloss_dypred = -2.0 / N * (y - y_pred)\n",
        "    dypred_dlinpred = np.exp(-lin_pred) * (1 / np.square(1 + np.exp(-1 * lin_pred)))\n",
        "    dlinpred_dw = x[:,0]\n",
        "    dlinpred_db = 1\n",
        "\n",
        "    # Calculate gradients and update weight and bias parameters\n",
        "    grad_w = (dloss_dypred * dypred_dlinpred * dlinpred_dw).sum()\n",
        "    grad_b = (dloss_dypred * dypred_dlinpred * dlinpred_db).sum()\n",
        "    w -= learning_rate * grad_w # calculate w^(k+1) from w^(k)\n",
        "    b -= learning_rate * grad_b # calculate b^(k+1) from b^(k)\n",
        "\n",
        "print(f\" w = {w.item():.4f}\")\n",
        "print(f\" b = {b.item():.4f}\")\n",
        "\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(18,4))\n",
        "ax1.plot(range(0,len(loss)), loss)\n",
        "ax1.set(xlabel=\"optimization iteration\", ylabel=\"loss\")\n",
        "ax2.plot(range(0,len(params)), [parm[0] for parm in params])\n",
        "ax2.set(xlabel=\"optimization iteration\", ylabel=\"parameter: w\")\n",
        "ax3.plot(range(0,len(params)), [parm[1] for parm in params])\n",
        "ax3.set(xlabel=\"optimization iteration\", ylabel=\"parameter: b\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auvsj_BrP7eY"
      },
      "source": [
        "***\n",
        "### Q3: Try running the above code cell a few times. Do you get the exact same parameter value estimates for $b$ and $w$ each time? Explain why or why not?\n",
        "\n",
        "\\<INPUT YOUR ANSWER TO Q3 HERE\\>\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdqoV8FVP7eY"
      },
      "source": [
        "### 4. Simple Perceptron w/ PyTorch\n",
        "\n",
        "Now we will train a basic perceptron using PyTorch to take advantage of its ability to perform automatic differentation. This allows us to avoid calculating derivatives manually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nskRodu3P7eY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Randomly initialize weights and other data\n",
        "#torch.manual_seed(42)\n",
        "w = torch.randn(1, requires_grad=True)\n",
        "b = torch.randn(1, requires_grad=True)\n",
        "x_tensor = torch.tensor(x)\n",
        "y_tensor = torch.tensor(y)\n",
        "learning_rate = 5e-1\n",
        "losses = []\n",
        "params = []\n",
        "\n",
        "# Carry out gradient descent\n",
        "for i in range(250):\n",
        "\n",
        "    params.append((w.item(), b.item()))\n",
        "\n",
        "    # Forward pass: compute predicted y\n",
        "    lin_pred = w * x_tensor + b\n",
        "    y_pred = lin_pred.flatten().sigmoid()\n",
        "\n",
        "    # Compute and store loss, and print occassionally\n",
        "    loss = 1/N * (y_tensor - y_pred).pow(2).sum()\n",
        "    losses.append(loss.item())\n",
        "    if i % 50 == 0:\n",
        "        print(f\"iteration {i}: loss = {loss.item():.4f}, w = {w[0]:.4f}, b = {b[0]:.4f}\")\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    # Update parameters but don't include these calculations as part of underlying computational graph\n",
        "    with torch.no_grad():\n",
        "        w -= learning_rate * w.grad\n",
        "        b -= learning_rate * b.grad\n",
        "\n",
        "    # Reset gradients for next iteration\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "\n",
        "print(f\" w = {w.item():.4f}\")\n",
        "print(f\" b = {b.item():.4f}\")\n",
        "\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(18,4))\n",
        "ax1.plot(range(0,len(losses)), losses)\n",
        "ax1.set(xlabel=\"optimization iteration\", ylabel=\"loss\")\n",
        "ax2.plot(range(0,len(params)), [parm[0] for parm in params])\n",
        "ax2.set(xlabel=\"optimization iteration\", ylabel=\"parameter: w\")\n",
        "ax3.plot(range(0,len(params)), [parm[1] for parm in params])\n",
        "ax3.set(xlabel=\"optimization iteration\", ylabel=\"parameter: b\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAtNgNR-P7eY"
      },
      "source": [
        "***\n",
        "### Q4: Run the above code a cell a few times and then try to read it to understand what is happening. Specifically, state which line(s) is calculating the gradients?\n",
        "Note that the gradients are the derivatives of the objective function with respect to the model parameters (that are then evaluated at the current values of the parameters).\n",
        "\n",
        "\n",
        "\\<INPUT YOUR ANSWER TO Q4 HERE\\>\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmQdpHyPP7eY"
      },
      "source": [
        "### 5: Defining a Perceptron w/ PyTorch\n",
        "\n",
        "Now we will train a basic perceptron using PyTorch to define the model architecture itself, while continuing to take advantage of its automatic differentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCWn7SsSP7eY"
      },
      "outputs": [],
      "source": [
        "# Randomly initialize weights and other data\n",
        "torch.manual_seed(42)\n",
        "#del w #= torch.randn(1, requires_grad=True).reshape(1,1)\n",
        "#del b #b = torch.randn(1, requires_grad=True).reshape(1,1)\n",
        "x_tensor = torch.tensor(x).float()\n",
        "y_tensor = torch.tensor(y).float().reshape(N, D_out)\n",
        "\n",
        "# Define and declare a pytorch perceptron using sigmoid activation function\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(1, 1), # linear layer\n",
        "    torch.nn.Sigmoid()     # activation function\n",
        ")\n",
        "\n",
        "# Define loss function to be used\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "learning_rate = 5e-1\n",
        "losses = []\n",
        "params = []\n",
        "\n",
        "# Carry out gradient descent\n",
        "for i in range(250):\n",
        "\n",
        "    params.append(tuple([param.item() for param in model.parameters()]))\n",
        "\n",
        "    # Forward pass: compute predicted y\n",
        "    y_pred = model.forward(x_tensor)\n",
        "\n",
        "    # Compute and store loss, and print occassionally\n",
        "    loss = loss_fn(y_pred, y_tensor)\n",
        "    losses.append(loss.item())\n",
        "    if i % 50 == 0:\n",
        "        print(f\"iteration {i}: loss = {loss.item():.4f}\")\n",
        "\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Backprop using PyTorch's automatic differentiation\n",
        "    loss.backward()\n",
        "\n",
        "    # Update parameters but don't include these calculations as part of underlying computational graph\n",
        "    with torch.no_grad():\n",
        "        for param in model.parameters():\n",
        "            param.data -= learning_rate * param.grad\n",
        "\n",
        "print(\"parameter values after training:\")\n",
        "for i, param in enumerate(model.parameters()):\n",
        "    print(f\"  param {i} est = {param.item():.4f}\")\n",
        "\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(18,4))\n",
        "ax1.plot(range(0,len(losses)), losses)\n",
        "ax1.set(xlabel=\"optimization iteration\", ylabel=\"loss\")\n",
        "ax2.plot(range(0,len(params)), [parm[0] for parm in params])\n",
        "ax2.set(xlabel=\"optimization iteration\", ylabel=\"parameter: w\")\n",
        "ax3.plot(range(0,len(params)), [parm[1] for parm in params])\n",
        "ax3.set(xlabel=\"optimization iteration\", ylabel=\"parameter: b\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36ohEEJFP7eY"
      },
      "source": [
        "***\n",
        "### Q5: Run the above code a cell a few times and then try to read it to understand what is happening. Specifically, state which line(s) is carrying out the Gradient Descent update?\n",
        "\n",
        "\n",
        "\\<INPUT YOUR ANSWER TO Q5 HERE\\>\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTBva4V1P7ea"
      },
      "source": [
        "### 6. Using PyTorch Optimizer\n",
        "\n",
        "Now we will train a basic perceptron using PyTorch to define the model itself and also to carry out the Adam optimization method (a extension of gradient descent that adaptively estimates gradients to be able to find minima more quickly).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFv_LabwP7ea"
      },
      "outputs": [],
      "source": [
        "# Randomly initialize weights and other data\n",
        "torch.manual_seed(42)\n",
        "x_tensor = torch.tensor(x).float()\n",
        "y_tensor = torch.tensor(y).float().reshape(N, D_out)\n",
        "\n",
        "# Declare a perceptron instance\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(D_in, D_out),\n",
        "    torch.nn.Sigmoid()\n",
        ")\n",
        "\n",
        "# Define loss function to be used\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "#loss_fn = torch.nn.BCELoss()\n",
        "\n",
        "learning_rate = 0.05\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "losses = []\n",
        "params = []\n",
        "\n",
        "# Carry out gradient descent\n",
        "for i in range(200 + 1):\n",
        "\n",
        "    params.append(tuple([param.item() for param in model.parameters()]))\n",
        "\n",
        "    # Forward pass: compute predicted y\n",
        "    y_pred = model.forward(x_tensor)\n",
        "\n",
        "    # Compute and store loss, and print occassionally\n",
        "    loss = loss_fn(y_pred, y_tensor)\n",
        "    losses.append(loss.item())\n",
        "    if i % 50 == 0:\n",
        "        print(f\"iteration {i}: loss = {loss.item():.4f}\")\n",
        "\n",
        "    # Zero all gradients before backward pass\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Backprop then call optimizer step to update all (relevant) model parameters for us\n",
        "    loss.backward()\n",
        "    optimizer.step() # this calculates w(k+1) for us (for all parameters)\n",
        "\n",
        "print(\"w and b estimates:\")\n",
        "for param in model.parameters():\n",
        "    print(f\"    {param.item():.4f}\")\n",
        "\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(18,4))\n",
        "ax1.plot(range(0,len(losses)), losses)\n",
        "ax1.set(xlabel=\"optimization iteration\", ylabel=\"loss\")\n",
        "ax2.plot(range(0,len(params)), [parm[0] for parm in params])\n",
        "ax2.set(xlabel=\"optimization iteration\", ylabel=\"parameter: w\")\n",
        "ax3.plot(range(0,len(params)), [parm[1] for parm in params])\n",
        "ax3.set(xlabel=\"optimization iteration\", ylabel=\"parameter: b\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2YgzHkqP7ea"
      },
      "source": [
        "***\n",
        "### Q6: Run the above code a cell a few times and then try changing the learning rate above to a larger value. How many epochs are then needed to get the loss below 0.10? Is it possible to get the loss below 0.05?\n",
        "\n",
        "\\<INPUT YOUR ANSWER TO Q6 HERE\\>\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHtCsiLWP7ea"
      },
      "source": [
        "\n",
        "### 6. Using PyTorch's torch.nn.Module Base Class\n",
        "Finally, let's make use of the PyTorch base class for creating a neural network. The `torch.nn.Module` class is the base class that we will inherit from to create a neural network. Notice that to start we override the constructor, `__init__`, and that inside this method we define the various layers to our neural network. We can then define the `forward` method to behave specifically how we choose. In the example below we pass an additional flag to the `forward()` method to either calculate and return the final predicted probability (with `apply_sigmoid=True`) or to simply return the linear predictor (with `apply_sigmoid=False`). This can be useful for us in some instances. For example, the log-loss function (aka binary cross-entropy) can execute slightly more efficiently by taking advantage of the fact that the `log` and `exp` functions are inverses of one another (recall that the sigmoid function has `exp()` in it)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tN8_zWbRP7ea"
      },
      "outputs": [],
      "source": [
        "# Randomly initialize weights and other data\n",
        "torch.manual_seed(42)\n",
        "#w = torch.randn(1, requires_grad=True).reshape(1,1)\n",
        "#b = torch.randn(1, requires_grad=True).reshape(1,1)\n",
        "x_tensor = torch.tensor(x).float()\n",
        "y_tensor = torch.tensor(y).float().reshape(N, D_out)\n",
        "\n",
        "# Define our own ANN class\n",
        "class Perceptron(torch.nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Perceptron, self).__init__()\n",
        "        self.lay1 = torch.nn.Linear(input_dim, 1) # linear layer\n",
        "        self.act = torch.nn.Sigmoid()             # activation layer\n",
        "    def forward(self, x, apply_sigmoid=False):\n",
        "        output = self.lay1(x)\n",
        "        if apply_sigmoid:\n",
        "            output = self.act(output)\n",
        "        return output\n",
        "\n",
        "model = Perceptron(1)\n",
        "\n",
        "# Define loss function to be used\n",
        "#loss_fn = torch.nn.MSELoss()\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "#loss_fn = torch.nn.BCEWithLogitsLoss() # expects only linear predictor (w/o sigmoid applied)\n",
        "learning_rate = 5e-1\n",
        "\n",
        "# define optimizer to be used and tell it about the model's parameters\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "losses = []\n",
        "params = []\n",
        "\n",
        "# Carry out gradient descent\n",
        "for i in range(200+1):\n",
        "\n",
        "    params.append(tuple([param.item() for param in model.parameters()]))\n",
        "\n",
        "    # Forward pass: compute predicted y\n",
        "    y_pred = model.forward(x_tensor, apply_sigmoid=True) #False)\n",
        "\n",
        "    # Compute and store loss, and print occassionally\n",
        "    loss = loss_fn(y_pred, y_tensor)\n",
        "\n",
        "    losses.append(loss.item())\n",
        "    if i % 50 == 0:\n",
        "        print(f\"iteration {i}: loss = {loss.item():.4f}\") #\" w = {w[0]:.4f}, b = {b[0]:.4f}\")\n",
        "\n",
        "    # Zero all gradients before backward pass\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Backpropagation: calculate loss for each model parameter and have optimizer update all of them\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(\"w and b estimates:\")\n",
        "for param in model.parameters():\n",
        "    print(f\"   {param.item():.4f}\")\n",
        "\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(18,4))\n",
        "ax1.plot(range(0,len(losses)), losses)\n",
        "ax1.set(xlabel=\"optimization iteration\", ylabel=\"loss\")\n",
        "ax2.plot(range(0,len(params)), [parm[0] for parm in params])\n",
        "ax2.set(xlabel=\"optimization iteration\", ylabel=\"parameter: w\")\n",
        "ax3.plot(range(0,len(params)), [parm[1] for parm in params])\n",
        "ax3.set(xlabel=\"optimization iteration\", ylabel=\"parameter: b\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_i683lZIP7ea"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py312",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}